{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1YIOxHhgf3V65RZ8oPDU0a2bOy_RLqaSS",
      "authorship_tag": "ABX9TyMwXXl49EMQvY0iZgmNjAki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chmorfop/modified_GIT/blob/main/GIT_preprossesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intro"
      ],
      "metadata": {
        "id": "8Ufs--KTN9dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = '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'\n",
        "username = 'chmorfop'\n",
        "repo = 'modified_GIT'\n"
      ],
      "metadata": {
        "id": "UC7DjRsXyWVz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{token}@github.com/{username}/{repo}\n"
      ],
      "metadata": {
        "id": "4kAOoSuY7KAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathy ='/content/drive/MyDrive/Colab Notebooks/test_data/val2014.zip'"
      ],
      "metadata": {
        "id": "ocr_xAwE7J9h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip -q /content/drive/MyDrive/\"Colab Notebooks\"/test_data/val2014.zip -d ./output "
      ],
      "metadata": {
        "id": "63fIQV7R7J6_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd modified_GIT/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj5x6L05Al8_",
        "outputId": "8264a2d2-10e2-40f0-d30d-c96cbc907d58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modified_GIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxgI8uOJ-08U",
        "outputId": "ceb9c25e-7989-4c27-b098-83077bc8d47f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for azfuse (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Libraries"
      ],
      "metadata": {
        "id": "8gFJbazmOWSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch import optim\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BertTokenizer\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def display_image(img,my_title=None):\n",
        "  '''\n",
        "  Custom function to display single image with its title\n",
        "  '''\n",
        "  fig, axes = plt.subplots(1, figsize=(15,15))\n",
        "  if len(img.shape) == 2 : # grayscale , only 1 channel\n",
        "    plt.imshow(img,cmap='gray')\n",
        "  else:\n",
        "    plt.imshow(img)\n",
        "  if my_title is not None:\n",
        "    plt.title(my_title)\n",
        "  plt.axis('off');"
      ],
      "metadata": {
        "id": "2_m_O21lJqtK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pathy ='/content/drive/MyDrive/Colab Notebooks/test_data/'\n",
        "f = open(pathy + 'captions_val2014.json')\n",
        "data = json.load(f)\n",
        "mylimit = 50\n",
        "subdata = data['annotations'][:mylimit]\n",
        "skliros = '/content/output'\n",
        "\n",
        "myimage_ids = []\n",
        "mycaptions = []\n",
        "for s in subdata:\n",
        "    filename = f\"/val2014/COCO_val2014_{int(s['image_id']):012d}.jpg\"\n",
        "    myimage_ids.append(skliros + filename)\n",
        "    mycaptions.append(s['caption'])\n",
        "\n",
        "\n",
        "print(len(myimage_ids))\n",
        "print(len(mycaptions))\n"
      ],
      "metadata": {
        "id": "FOIL2H4cJvsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173c2d3f-d91d-4f37-ea63-caa90aeb917b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################"
      ],
      "metadata": {
        "id": "4cgK1_76UNI4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def recursive_to_device(d, device, **kwargs):\n",
        "    if isinstance(d, tuple) or isinstance(d, list):\n",
        "        return [recursive_to_device(x, device, **kwargs) for x in d]\n",
        "    elif isinstance(d, dict):\n",
        "        return dict((k, recursive_to_device(v, device)) for k, v in d.items())\n",
        "    elif isinstance(d, torch.Tensor) or hasattr(d, 'to'):\n",
        "        #return d.to(device, non_blocking=True)\n",
        "        return d.to(device, **kwargs)\n",
        "    else:\n",
        "        return \n",
        "\n",
        "def collate_fn(batch):\n",
        "    # arrange in proper order\n",
        "    # this function is designed to support any customized type and to be compatible\n",
        "    # with the default collate function\n",
        "    ele = batch[0]\n",
        "    if isinstance(ele, dict):\n",
        "        return {key: collate_fn([d[key] for d in batch]) for key in ele}\n",
        "    elif isinstance(ele, (tuple, list)):\n",
        "        return [collate_fn(x) for x in zip(*batch)]\n",
        "    else:\n",
        "        if all(isinstance(b, torch.Tensor) for b in batch) and len(batch) > 0:\n",
        "            if not all(b.shape == batch[0].shape for b in batch[1:]):\n",
        "                assert all(len(b.shape) == len(batch[0].shape) for b in batch[1:])\n",
        "                shape = torch.tensor([b.shape for b in batch])\n",
        "                max_shape = tuple(shape.max(dim=0)[0].tolist())\n",
        "                batch2 = []\n",
        "                for b in batch:\n",
        "                    if any(c < m for c, m in zip(b.shape, max_shape)):\n",
        "                        b2 = torch.zeros(max_shape, dtype=b.dtype, device=b.device)\n",
        "                        if b.dim() == 1:\n",
        "                            b2[:b.shape[0]] = b\n",
        "                        elif b.dim() == 2:\n",
        "                            b2[:b.shape[0], :b.shape[1]] = b\n",
        "                        elif b.dim() == 3:\n",
        "                            b2[:b.shape[0], :b.shape[1], :b.shape[2]] = b\n",
        "                        else:\n",
        "                            raise NotImplementedError\n",
        "                        b = b2\n",
        "                    batch2.append(b)\n",
        "                batch = batch2\n",
        "        return default_collate(batch)"
      ],
      "metadata": {
        "id": "gnPEWJEfSsUE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### GET DATA ################\n",
        "\n",
        "def load_image_by_pil(file_name, respect_exif=False):\n",
        "    # '../'\n",
        "    #temp = '../'\n",
        "    if isinstance(file_name, str):\n",
        "        image = Image.open( file_name).convert('RGB')\n",
        "    elif isinstance(file_name, bytes):\n",
        "        import io\n",
        "        image = Image.open(io.BytesIO(file_name)).convert('RGB')\n",
        "    if respect_exif:\n",
        "        from PIL import ImageOps\n",
        "        image = ImageOps.exif_transpose(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def get_data(image_file, prefix, target, tokenizer, image_transform):\n",
        "    max_text_len = 40\n",
        "    # prefix encoding --- none for IC\n",
        "    prefix_encoding = tokenizer(\n",
        "        prefix, padding='do_not_pad',\n",
        "        add_special_tokens=False,\n",
        "        truncation=True, max_length=max_text_len)\n",
        "    # caption - target encoding  -- input ids - token type ids - attention mask\n",
        "    # 1012 i teleia sto decode\n",
        "    target_encoding = tokenizer(\n",
        "        target, padding='do_not_pad',\n",
        "        add_special_tokens=False,\n",
        "        truncation=True, max_length=max_text_len)\n",
        "    # len of target + 1 , i teleia\n",
        "    # need predict [0,0,0,1,1,1,1,1] i [1,1,1,1,]\n",
        "    need_predict = [0] * len(prefix_encoding['input_ids']) + [1] * len(target_encoding['input_ids'])\n",
        "    # payload sum of input ids\n",
        "    payload = prefix_encoding['input_ids'] + target_encoding['input_ids']\n",
        "\n",
        "    if len(payload) > max_text_len:\n",
        "        payload = payload[-(max_text_len - 2):]\n",
        "        need_predict = need_predict[-(max_text_len - 2):]\n",
        "\n",
        "    # CLS - 101 .... SEP - 102\n",
        "    input_ids = [tokenizer.cls_token_id] + payload + [tokenizer.sep_token_id]\n",
        "    need_predict = [0] + need_predict + [1]\n",
        "\n",
        "    im = load_image_by_pil(image_file)\n",
        "    # print('*'*8)\n",
        "    # print(im)\n",
        "    # print('*'*8)\n",
        "    data = {\n",
        "        'caption_tokens': torch.tensor(input_ids),\n",
        "        'need_predict': torch.tensor(need_predict),\n",
        "        'image': im,\n",
        "        'caption': {},\n",
        "        # this iteration can be used for crop-size selection so that all GPUs\n",
        "        # can process the image with the same input size\n",
        "        'iteration': 0,\n",
        "    }\n",
        "\n",
        "\n",
        "    data = image_transform(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "ijiJ0O3LQH7c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_path(d, with_type=False, leaf_only=True, with_list=True):\n",
        "    assert not with_type, 'will not support'\n",
        "    all_path = []\n",
        "\n",
        "    if isinstance(d, dict):\n",
        "        for k, v in d.items():\n",
        "            all_sub_path = get_all_path(\n",
        "                v, with_type, leaf_only=leaf_only, with_list=with_list)\n",
        "            all_path.extend([k + '$' + p for p in all_sub_path])\n",
        "            if not leaf_only or len(all_sub_path) == 0:\n",
        "                all_path.append(k)\n",
        "    elif (isinstance(d, tuple) or isinstance(d, list)) and with_list:\n",
        "        for i, _v in enumerate(d):\n",
        "            all_sub_path = get_all_path(\n",
        "                _v, with_type,\n",
        "                leaf_only=leaf_only,\n",
        "                with_list=with_list,\n",
        "            )\n",
        "            all_path.extend(['{}$'.format(i) + p for p in all_sub_path])\n",
        "            if not leaf_only or len(all_sub_path) == 0:\n",
        "                all_path.append('{}'.format(i))\n",
        "    return all_path\n",
        "\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self, default, overwrite):\n",
        "        self.default = default\n",
        "        self.overwrite = overwrite\n",
        "\n",
        "    def get(self, k):\n",
        "        if dict_has_path(self.default, k):\n",
        "            base = dict_get_path_value(self.default, k)\n",
        "        else:\n",
        "            base = None\n",
        "        if dict_has_path(self.overwrite, k):\n",
        "            over = dict_get_path_value(self.overwrite, k)\n",
        "            if isinstance(base, dict):\n",
        "                assert isinstance(over, dict)\n",
        "                base.update(over)\n",
        "            else:\n",
        "                base = over\n",
        "        return base\n",
        "\n",
        "    def __getattr__(self, k):\n",
        "        return self.get(k)\n",
        "\n",
        "    def __copy__(self):\n",
        "        return Config(self.default, self.overwrite)\n",
        "\n",
        "    def __deepcopy__(self, memo):\n",
        "        from copy import deepcopy\n",
        "        return Config(deepcopy(self.default), deepcopy(self.overwrite))\n",
        "\n",
        "    def get_dict(self):\n",
        "        import copy\n",
        "        default = copy.deepcopy(self.default)\n",
        "        for p in get_all_path(self.overwrite, with_list=False):\n",
        "            v = dict_get_path_value(self.overwrite, p)\n",
        "            dict_update_path_value(default, p, v)\n",
        "        return default"
      ],
      "metadata": {
        "id": "DRM-C1u7Zmbx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_remove_path(d, p):\n",
        "    ps = p.split('$')\n",
        "    assert len(ps) > 0\n",
        "    cur_dict = d\n",
        "    need_delete = ()\n",
        "    while True:\n",
        "        if len(ps) == 1:\n",
        "            if len(need_delete) > 0 and len(cur_dict) == 1:\n",
        "                del need_delete[0][need_delete[1]]\n",
        "            else:\n",
        "                del cur_dict[ps[0]]\n",
        "            return\n",
        "        else:\n",
        "            if len(cur_dict) == 1:\n",
        "                if len(need_delete) == 0:\n",
        "                    need_delete = (cur_dict, ps[0])\n",
        "            else:\n",
        "                need_delete = (cur_dict, ps[0])\n",
        "            cur_dict = cur_dict[ps[0]]\n",
        "            ps = ps[1:]\n",
        "\n",
        "def dict_parse_key(k, with_type):\n",
        "    if with_type:\n",
        "        if k[0] == 'i':\n",
        "            return int(k[1:])\n",
        "        else:\n",
        "            return k[1:]\n",
        "    return k\n",
        "\n",
        "def dict_has_path(d, p, with_type=False):\n",
        "    ps = p.split('$')\n",
        "    cur_dict = d\n",
        "    while True:\n",
        "        if len(ps) > 0:\n",
        "            k = dict_parse_key(ps[0], with_type)\n",
        "            if isinstance(cur_dict, dict) and k in cur_dict:\n",
        "                cur_dict = cur_dict[k]\n",
        "                ps = ps[1:]\n",
        "            elif isinstance(cur_dict, list):\n",
        "                try:\n",
        "                    k = int(k)\n",
        "                except:\n",
        "                    return False\n",
        "                cur_dict = cur_dict[k]\n",
        "                ps = ps[1:]\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "\n",
        "def dict_update_path_value(d, p, v):\n",
        "    ps = p.split('$')\n",
        "    while True:\n",
        "        if len(ps) == 1:\n",
        "            d[ps[0]] = v\n",
        "            break\n",
        "        else:\n",
        "            if ps[0] not in d:\n",
        "                d[ps[0]] = {}\n",
        "            d = d[ps[0]]\n",
        "            ps = ps[1:]\n",
        "\n",
        "\n",
        "def dict_get_path_value(d, p, with_type=False):\n",
        "    ps = p.split('$')\n",
        "    cur_dict = d\n",
        "    while True:\n",
        "        if len(ps) > 0:\n",
        "            k = dict_parse_key(ps[0], with_type)\n",
        "            if isinstance(cur_dict, (tuple, list)):\n",
        "                cur_dict = cur_dict[int(k)]\n",
        "            else:\n",
        "                cur_dict = cur_dict[k]\n",
        "            ps = ps[1:]\n",
        "        else:\n",
        "            return cur_dict"
      ],
      "metadata": {
        "id": "qHe8H48vZJY3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class RenameKey(object):\n",
        "    def __init__(self, ft=None, not_delete_origin=False):\n",
        "        self.ft = ft\n",
        "        self.not_delete_origin = not_delete_origin\n",
        "    def __repr__(self):\n",
        "        return 'RenameKey(ft={}, not_delete_origin={})'.format(\n",
        "            ','.join(['{}:{}'.format(k, v) for k, v in self.ft.items()]),\n",
        "            self.not_delete_origin,\n",
        "        )\n",
        "    def __call__(self, data):\n",
        "        if self.ft is None:\n",
        "            return data\n",
        "        for k, k1 in self.ft.items():\n",
        "            # we should not fall to the situation where some data has some key\n",
        "            # and some data has not some key. We should either have a key or\n",
        "            # not for all data consistently. Thus, for re-naming, we should not\n",
        "            # to check whether it has or not. it should always have that key.\n",
        "            # otherwise, we should not specify it.\n",
        "            #if dict_has_path(data, k):\n",
        "            if dict_has_path(data, k):\n",
        "                v = dict_get_path_value(data, k)\n",
        "                dict_update_path_value(data, k1, v)\n",
        "                if not self.not_delete_origin:\n",
        "                    dict_remove_path(data, k)\n",
        "        return data\n",
        "\n",
        "class SelectTransform(object):\n",
        "    def __init__(self, ts, selector):\n",
        "        self.ts = ts\n",
        "        self.selector = selector\n",
        "    def __repr__(self):\n",
        "        return 'SelectTransform(ts={}, selector={})'.format(\n",
        "            self.ts, self.selector\n",
        "        )\n",
        "    def __call__(self, data):\n",
        "        idx = self.selector(data)\n",
        "        return self.ts[idx](data)\n",
        "\n",
        "class ImageTransform2Dict(object):\n",
        "    def __init__(self, image_transform, key='image'):\n",
        "        self.image_transform = image_transform\n",
        "        self.key = key\n",
        "\n",
        "    def __call__(self, dict_data):\n",
        "        out = dict(dict_data.items())\n",
        "        out[self.key] = self.image_transform(dict_data[self.key])\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'ImageTransform2Dict(image_transform={})'.format(\n",
        "            self.image_transform,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageTransform2Dict(object):\n",
        "    def __init__(self, image_transform, key='image'):\n",
        "        self.image_transform = image_transform\n",
        "        self.key = key\n",
        "\n",
        "    def __call__(self, dict_data):\n",
        "        out = dict(dict_data.items())\n",
        "        out[self.key] = self.image_transform(dict_data[self.key])\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'ImageTransform2Dict(image_transform={})'.format(\n",
        "            self.image_transform,\n",
        "        )\n",
        "\n",
        "def get_inception_train_transform(\n",
        "    bgr2rgb=False,\n",
        "    crop_size=224,\n",
        "    small_scale=None,\n",
        "    normalize=None,\n",
        "    no_color_jitter=False,\n",
        "    no_flip=False,\n",
        "    no_aspect_dist=False,\n",
        "    resize_crop=None,\n",
        "    max_size=None,\n",
        "    interpolation=Image.BILINEAR,\n",
        "):\n",
        "    if interpolation is None:\n",
        "        interpolation = Image.BILINEAR\n",
        "    elif interpolation == 'bicubic':\n",
        "        interpolation = Image.BICUBIC\n",
        "    totensor = transforms.ToTensor()\n",
        "    all_trans = []\n",
        "    if small_scale is None:\n",
        "        small_scale = 0.08\n",
        "    scale = (small_scale, 1.)\n",
        "    if no_aspect_dist:\n",
        "        ratio = (1., 1.)\n",
        "    else:\n",
        "        ratio = (3. / 4., 4. / 3.)\n",
        "    if resize_crop is None:\n",
        "        all_trans.append(transforms.RandomResizedCrop(\n",
        "            crop_size,\n",
        "            scale=scale,\n",
        "            ratio=ratio,\n",
        "            interpolation=interpolation,\n",
        "        ))\n",
        "    else:\n",
        "        raise NotImplementedError(resize_crop)\n",
        "\n",
        "    if not no_color_jitter:\n",
        "        color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4)\n",
        "        all_trans.append(color_jitter)\n",
        "\n",
        "    if not no_flip:\n",
        "        all_trans.append(transforms.RandomHorizontalFlip())\n",
        "\n",
        "    all_trans.extend([\n",
        "        totensor,\n",
        "        normalize,])\n",
        "    data_augmentation = transforms.Compose(all_trans)\n",
        "    return data_augmentation\n",
        "\n",
        "def get_transform_image(cfg, is_train):\n",
        "    train_transform = cfg.train_transform\n",
        "    if train_transform == 'vitp':\n",
        "        transform = get_transform_vit_default(\n",
        "            cfg, is_train=is_train)\n",
        "    else:\n",
        "        raise NotImplementedError(train_transform)\n",
        "    return transform\n",
        "\n",
        "def trans_select_for_crop_size(\n",
        "    data, train_crop_sizes,\n",
        "    iteration_multi=0,\n",
        "):\n",
        "    if iteration_multi <= 0:\n",
        "        if len(train_crop_sizes) == 1:\n",
        "            idx = 0\n",
        "        else:\n",
        "            idx = data['iteration'] % len(train_crop_sizes)\n",
        "    elif data['iteration'] <= iteration_multi:\n",
        "        idx = data['iteration'] % len(train_crop_sizes)\n",
        "    else:\n",
        "        idx = -1\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_multi_scale_image_transform(cfg, is_train, get_one=get_transform_image):\n",
        "    def get_multi_res_transform(s):\n",
        "        old = cfg.train_crop_size if is_train else cfg.test_crop_size\n",
        "        all_t = []\n",
        "        multi_res_factors = cfg.multi_res_factors or []\n",
        "        for i, f in enumerate(multi_res_factors):\n",
        "            if is_train:\n",
        "                cfg.train_crop_size = s // f\n",
        "            else:\n",
        "                cfg.test_crop_size = s // f\n",
        "            key = 'image_{}'.format(i)\n",
        "            all_t.append(RenameKey({'image': key}, not_delete_origin=True))\n",
        "            t = get_one(cfg, is_train)\n",
        "            t = ImageTransform2Dict(t, key=key)\n",
        "            all_t.append(t)\n",
        "        # get_one depends on train_crop_size\n",
        "        if is_train:\n",
        "            cfg.train_crop_size = s\n",
        "        else:\n",
        "            cfg.test_crop_size = s\n",
        "        t = get_one(cfg, is_train)\n",
        "        t = ImageTransform2Dict(t)\n",
        "        all_t.append(t)\n",
        "        if is_train:\n",
        "            cfg.train_crop_size = old\n",
        "        else:\n",
        "            cfg.test_crop_size = old\n",
        "        return transforms.Compose(all_t)\n",
        "\n",
        "   # min size range [160-224]\n",
        "   # train crop sizes [160-176-192-208 -224]\n",
        "    if is_train:\n",
        "        if cfg.min_size_range32 is None:\n",
        "            train_crop_sizes = [cfg.train_crop_size]\n",
        "        else:\n",
        "            train_crop_sizes = list(range(\n",
        "                cfg.min_size_range32[0],\n",
        "                cfg.min_size_range32[1] + cfg.patch_size - 1, cfg.patch_size,\n",
        "            ))\n",
        "    else:\n",
        "        train_crop_sizes = [cfg.test_crop_size]\n",
        "\n",
        "    crop_trans = []\n",
        "    for s in train_crop_sizes:\n",
        "        t = get_multi_res_transform(s)\n",
        "        crop_trans.append(t)\n",
        "    iteration_multi = 0\n",
        "    image_transform = SelectTransform(\n",
        "        crop_trans,\n",
        "        lambda d: trans_select_for_crop_size(\n",
        "            d, train_crop_sizes, iteration_multi))\n",
        "    return image_transform\n",
        "\n",
        "def get_image_transform(cfg):\n",
        "    return get_multi_scale_image_transform(cfg, is_train=True)\n",
        "\n",
        "def get_default_mean():\n",
        "    return [0.485, 0.456, 0.406]\n",
        "\n",
        "def get_default_std():\n",
        "    return [0.229, 0.224, 0.225]\n",
        "\n",
        "def get_transform_image_norm(cfg, default=None):\n",
        "    if cfg.data_normalize == 'default':\n",
        "        normalize = transforms.Normalize(\n",
        "            mean=get_default_mean(), std=get_default_std())\n",
        "    elif cfg.data_normalize == 'clip':\n",
        "        # clip model\n",
        "        normalize = transforms.Normalize(\n",
        "            (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    else:\n",
        "        raise NotImplementedError(cfg.data_normalize)\n",
        "    return normalize\n",
        "\n",
        "def get_transform_vit_default(cfg, is_train):\n",
        "    default_normalize = transforms.Normalize(\n",
        "            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    normalize = get_transform_image_norm(cfg, default_normalize)\n",
        "    transform = get_inception_train_transform(\n",
        "        bgr2rgb=True,\n",
        "        crop_size=cfg.train_crop_size,\n",
        "        normalize=normalize,\n",
        "        small_scale=cfg.input_small_scale,\n",
        "        no_color_jitter=cfg.no_color_jitter,\n",
        "        no_flip=cfg.no_flip,\n",
        "        no_aspect_dist=cfg.no_aspect_dist,\n",
        "        resize_crop=cfg.resize_crop,\n",
        "        max_size=cfg.train_max_size,\n",
        "        interpolation=cfg.interpolation or Image.BILINEAR,\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "def get_transform_image(cfg, is_train):\n",
        "    train_transform = cfg.train_transform\n",
        "    if train_transform == 'vitp':\n",
        "        transform = get_transform_vit_default(\n",
        "            cfg, is_train=is_train)\n",
        "    else:\n",
        "        raise NotImplementedError(train_transform)\n",
        "    return transform\n",
        "\n",
        "class ImageTransform2Images(object):\n",
        "    def __init__(self, sep_transform, first_joint=None):\n",
        "        self.image_transform = sep_transform\n",
        "        self.first_joint = first_joint\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        if self.first_joint is not None:\n",
        "            imgs = self.first_joint(imgs)\n",
        "        return [self.image_transform(im) for im in imgs]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'ImageTransform2Images(image_transform={})'.format(\n",
        "            self.image_transform,\n",
        "        )\n",
        "\n",
        "def get_transform_images(cfg, is_train):\n",
        "    trans = get_transform_image(cfg, is_train)\n",
        "    trans = ImageTransform2Images(trans)\n",
        "    return trans"
      ],
      "metadata": {
        "id": "gR0hYBF7Xtpi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = myimage_ids.copy()\n",
        "captions = mycaptions.copy()\n",
        "prefixs = None"
      ],
      "metadata": {
        "id": "vweQbQXFUwcu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if prefixs is None:\n",
        "    prefixs = [''] * len(captions)\n",
        "cfg = {\n",
        "    'crop_region_extend_in_datatransform': 4,\n",
        "    'data_normalize': 'clip',\n",
        "    'train_crop_size': 224,\n",
        "    'input_small_scale': 0.8,\n",
        "    'no_color_jitter': True,\n",
        "    'no_flip': True,\n",
        "    'no_aspect_dist': True,\n",
        "    'interpolation': 'bicubic',\n",
        "    'min_size_range32': [160, 224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
        "    'patch_size': 16,\n",
        "    'train_transform': 'vitp',\n",
        "}\n",
        "cfg = Config(cfg, {})\n",
        "all_data = []\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "image_transform = get_image_transform(cfg)\n",
        "\n",
        "\n",
        "\n",
        "for image_file, prefix, target in zip(image_files, prefixs, captions):\n",
        "    data = get_data(image_file, prefix, target,\n",
        "                    tokenizer, image_transform)\n",
        "    all_data.append(data)\n",
        "\n",
        "# data = collate_fn(all_data)\n",
        "# data = recursive_to_device(data, 'cpu')     # cuda\n",
        "\n",
        "# param = {}\n",
        "#model = get_git_model(tokenizer, param)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvJ8nYcKmLV",
        "outputId": "4defdbbe-b31b-4271-80dd-a59ce17c4a4d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:899: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4y8K_l35ejzh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a in all_data:\n",
        "  print(a.get('image').size())\n",
        "  # display_image(a.get('image').permute(1,2,0))\n",
        "  # print()\n"
      ],
      "metadata": {
        "id": "vrBoK54GKmCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jo8iPaFedV2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Github"
      ],
      "metadata": {
        "id": "IZfoUBhFVpVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -l"
      ],
      "metadata": {
        "id": "978utfwLJshV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59bd81d3-16aa-421d-ea6e-4493aad1ff1a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwx------ 5 root root 4096 Jan 26 08:20 \u001b[0m\u001b[01;34mdrive\u001b[0m/\n",
            "drwxr-xr-x 7 root root 4096 Jan 26 08:22 \u001b[01;34mmodified_GIT\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4096 Jan 26 08:20 \u001b[01;34moutput\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Jan  9 14:36 \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd modified_GIT/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUY_Z4qnAtRO",
        "outputId": "af817ef3-1ff9-4a00-eca5-f02840889813"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modified_GIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwC00yVVIs6T",
        "outputId": "17187064-10d1-4a66-dd5c-a475211e7435"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:U7as6UecpyjyDKKpMlVoYJZPCHBV7RV5+DFU64ouUOg root@40edfbd3f483\n",
            "The key's randomart image is:\n",
            "+---[RSA 2048]----+\n",
            "|= +.....  .=...  |\n",
            "|.* .    . + +  . |\n",
            "|o.o.   o .oo o.  |\n",
            "|  o.. . o+ ...   |\n",
            "| . . . .S.o.  .  |\n",
            "|  .   E  ++...   |\n",
            "| .. .  .oo.o.    |\n",
            "|oo ..o..o.o      |\n",
            "|*.   oo..o.      |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9UHQAoRI1vw",
        "outputId": "dc408ea0-0c14-4e2d-8c3c-fef9eed4cf96"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC+ae14F72oRPcCQWFtDXs/bGwXFt/8VqXBFnNRPA1HMFNQSSsVwMD2uqtXmSwNH3dGx81zHFcFCn/BWn4EdVHhV0mvFiXXLAr+gwB9fuWN9K2fowJReXI3eTWEu/GfqhG+/rw1NVQtyBQowENhLptQnJcrLCW2YC9sIAHFd+C6e695G1XH9AyN6VDxCkEjnrvrjQVwQdMu3R7NeUZMZlW3bvhom/ch0lYUtWNFb6Wv11XhzjtaxJRBjpqT523k4eM3HoQ79YtjQU2tGe99voGn6sxM3/d0E9jG7+3/hzAz705kl5//7uX68497gz9yN0JNaSMhJuMbzTFtNmSyEpRF root@40edfbd3f483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEJVQwwcI1su",
        "outputId": "d72db783-4fb2-4761-8a85-0dd3f976cb13"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-babeld-881dd265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin git@github.com:chmorfop/modified_GIT.git"
      ],
      "metadata": {
        "id": "3f3asPAgI1qC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"chmorfop\"\n",
        "!git config --global user.email \"chmorfop@gmail.com\"\n",
        "!git add .\n",
        "!git commit -m 'remove from colab'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApF-NYOMI1ms",
        "outputId": "fc2fe62f-b476-45bb-e959-004365f83b39"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main b51cab2] remove from colab\n",
            " 8 files changed, 1 insertion(+), 630 deletions(-)\n",
            " delete mode 100644 CODE_OF_CONDUCT.md\n",
            " delete mode 100644 GIT.ipynb\n",
            " delete mode 100644 LICENSE\n",
            " delete mode 100644 MANIFEST.in\n",
            " delete mode 100644 README.md\n",
            " delete mode 100644 SECURITY.md\n",
            " delete mode 100644 SUPPORT.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0JhbfzcJLWT",
        "outputId": "326bcee6-5107-46a1-d91c-56f6ed549b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   generativeimage2text/train.py\u001b[m\n",
            "\t\u001b[31mmodified:   generativeimage2text/train_morf.py\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHHBdcBYJWZw",
        "outputId": "be639d52-02aa-4589-f2ec-4f8033a5142e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added the RSA host key for IP address '20.27.177.113' to the list of known hosts.\n",
            "Counting objects: 4, done.\n",
            "Delta compression using up to 4 threads.\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 360 bytes | 360.00 KiB/s, done.\n",
            "Total 4 (delta 3), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
            "To github.com:chmorfop/modified_GIT.git\n",
            "   8285403..b51cab2  main -> main\n",
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtqV8vheJhaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}